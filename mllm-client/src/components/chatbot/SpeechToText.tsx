import { useState, useEffect, useRef } from 'react';

const SPEECH_TIMEOUT = 2000; // 2ì´ˆ ì¹¨ë¬µ í›„ ì¸ì‹ ì¼ì‹œ ì¤‘ì§€

interface SpeechToTextProps {
  onTranscript: (text: string) => void;
}

export function SpeechToText({ onTranscript }: SpeechToTextProps) {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcript, setTranscript] = useState('');

  const recognitionRef = useRef<SpeechRecognition | null>(null);
  const timeoutIdRef = useRef<NodeJS.Timeout | null>(null);

  const isSpeechRecognitionSupported =
    'SpeechRecognition' in window || 'webkitSpeechRecognition' in window;

  useEffect(() => {
    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.stop();
      }
      if (timeoutIdRef.current) {
        clearTimeout(timeoutIdRef.current);
      }
    };
  }, []);

  const startRecording = () => {
    try {
      setIsRecording(true);
      setTranscript(''); // ìƒˆ ë…¹ìŒ ì‹œì‘ ì‹œ ê¸°ì¡´ í…ìŠ¤íŠ¸ ì´ˆê¸°í™”

      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognitionRef.current = new SpeechRecognition();

      // ì„¤ì • ìµœì í™”
      recognitionRef.current.lang = 'ko-KR';
      recognitionRef.current.continuous = true; // ì—°ì† ì¸ì‹ í™œì„±í™”
      recognitionRef.current.interimResults = true;
      recognitionRef.current.maxAlternatives = 3; // ì—¬ëŸ¬ ëŒ€ì²´ ê²°ê³¼ ì œê³µ

      // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
      recognitionRef.current.onresult = (event) => {
        // ì´ì „ íƒ€ì„ì•„ì›ƒ ì´ˆê¸°í™”
        if (timeoutIdRef.current) {
          clearTimeout(timeoutIdRef.current);
        }

        const currentTranscript = Array.from(event.results)
          .map(result => result[0].transcript)
          .join('');

        setTranscript(currentTranscript);

        // ì¤‘ê°„ ê²°ê³¼ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ í…ìŠ¤íŠ¸ ì „ë‹¬
        if (event.results[0].isFinal) {
          onTranscript(currentTranscript);
        }

        // ìŒì„± ê°ì§€ í›„ ì¼ì • ì‹œê°„ ëŒ€ê¸° íƒ€ì´ë¨¸ ì„¤ì •
        timeoutIdRef.current = setTimeout(() => {
          if (recognitionRef.current) {
            try {
              // ì¸ì‹ ì¢…ë£Œ í›„ ì ì‹œ ëŒ€ê¸°í•˜ë‹¤ê°€ ë‹¤ì‹œ ì‹œì‘
              recognitionRef.current.stop();
            } catch (e) {
              console.error("Error stopping recognition:", e);
            }
          }
        }, SPEECH_TIMEOUT); // 2ì´ˆ ì¹¨ë¬µ í›„ ì¸ì‹ ì¼ì‹œ ì¤‘ì§€
      };

      recognitionRef.current.onend = () => {
        // ì¸ì‹ì´ ì¢…ë£Œë˜ì—ˆì„ ë•Œ ìë™ìœ¼ë¡œ ë‹¤ì‹œ ì‹œì‘ (ì¤‘ì§€ ë²„íŠ¼ ëˆ„ë¥´ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
        if (isRecording && !isProcessing) {
          try {
            recognitionRef.current?.start();
          } catch (e) {
            console.error("Error restarting recognition:", e);
          }
        } else {
          setIsRecording(false);
          setIsProcessing(false);
        }
      };

      recognitionRef.current.onerror = (event) => {
        console.error('Speech recognition error:', event.error);

        // 'no-speech' ì—ëŸ¬ëŠ” ë¬´ì‹œí•˜ê³  ë‹¤ì‹œ ì‹œì‘
        if (event.error === 'no-speech') {
          try {
            setTimeout(() => {
              if (isRecording && recognitionRef.current) {
                recognitionRef.current.start();
              }
            }, 100);
          } catch (e) {
            console.error("Error restarting after no-speech:", e);
          }
        } else if (event.error === 'network') {
          // ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ì‹œ ì¬ì—°ê²° ì‹œë„
          setTimeout(() => {
            if (isRecording) {
              startRecording();
            }
          }, 1000);
        } else {
          setIsRecording(false);
          setIsProcessing(false);
        }
      };

      // ì¸ì‹ ì‹œì‘
      recognitionRef.current.start();
    } catch (error) {
      console.error('Failed to start speech recognition:', error);
      setIsRecording(false);
    }
  };

  const stopRecording = () => {
    setIsProcessing(true);

    // íƒ€ì„ì•„ì›ƒ ì •ë¦¬
    if (timeoutIdRef.current) {
      clearTimeout(timeoutIdRef.current);
      timeoutIdRef.current = null;
    }

    // ìµœì¢… í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì „ë‹¬
    if (transcript && transcript.trim() !== '') {
      onTranscript(transcript);
    }

    // ì¸ì‹ ì¤‘ì§€
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
  };

  if (!isSpeechRecognitionSupported) {
    return (
      <div className="speech-to-text">
        <button disabled title="ë¸Œë¼ìš°ì €ê°€ ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤">
          ìŒì„± ì…ë ¥ ë¶ˆê°€
        </button>
      </div>
    );
  }

  return (
    <div className="speech-to-text">
      <button
        onClick={isRecording ? stopRecording : startRecording}
        disabled={isProcessing}
        className={isRecording ? 'recording' : ''}
        aria-label={isRecording ? 'ë…¹ìŒ ì¤‘ì§€' : 'ìŒì„± ì…ë ¥'}
      >
        {isRecording ? 'ë…¹ìŒ ì¤‘ì§€' : isProcessing ? 'ì²˜ë¦¬ ì¤‘...' : 'ìŒì„± ì…ë ¥'}
      </button>
      {isRecording && <span className="recording-indicator">ğŸ”´</span>}
    </div>
  );
}